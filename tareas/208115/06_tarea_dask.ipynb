{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Instrucciones generales\n",
    "- Esta tarea debe realizarse de manera individual\n",
    "- Este notebook (resuelto) debe ser subido al github del proyecto en la carpeta de tareas (creen una carpeta dentro de esa carpeta y agreguen su notebook reuelto)\n",
    "- Fecha límite: Lunes 25 de noviembre de 2024 a las 11:59 p.m\n",
    "- Deben realizar las cuatro secciones\n",
    "- Puedes agregar tantas celdas de código y explicaciones como veas necesario, solo manten la estructura general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 0 Creación y Configuración del cliente de Dask\n",
    "Ejercicio 0: Configuración del cliente\n",
    "1. Crea un cliente local de Dask que inicie un clúster en tu máquina.\n",
    "2. Configura el cliente para que tenga las siguientes características (elige un par de las opciones de trabajadores e hilos):\n",
    "    - Número de trabajadores: 2 / 4\n",
    "    - Memoria máxima por trabajador: 1GB\n",
    "    - Threads por trabajador: 4 / 2\n",
    "3. Verifica que el cliente esté funcionando correctamente mostrando:\n",
    "    - Resumen de los trabajadores activos.\n",
    "    - Dashboard disponible (URL del panel de control de Dask).\n",
    "    * Tip: Checa los parámetros del cliente que creeaste.\n",
    "\n",
    "*Nota*: Puedes hacer que corra en el puerto que desees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 32835 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen de los trabajadores activos:\n",
      "<Client: 'tcp://127.0.0.1:35535' processes=2 threads=8, memory=1.86 GiB>\n",
      "\n",
      "URL del panel de control de Dask (Dashboard):\n",
      "http://127.0.0.1:32835/status\n",
      "\n",
      "Información detallada de los trabajadores:\n",
      "Trabajador: tcp://127.0.0.1:37499\n",
      "  Estado: running\n",
      "  Hilos: 4\n",
      "  Memoria usada (aproximada): 57507840 bytes\n",
      "  Memoria límite: 1000000000 bytes\n",
      "Trabajador: tcp://127.0.0.1:45771\n",
      "  Estado: running\n",
      "  Hilos: 4\n",
      "  Memoria usada (aproximada): 57524224 bytes\n",
      "  Memoria límite: 1000000000 bytes\n"
     ]
    }
   ],
   "source": [
    "# Tu código va aquí\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Configuración del clúster local\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2,              # Número de trabajadores\n",
    "    threads_per_worker=4,     # Hilos por trabajador\n",
    "    memory_limit=\"1GB\"        # Memoria máxima por trabajador\n",
    ")\n",
    "\n",
    "# Crear cliente conectado al clúster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Mostrar información del cliente y verificar su funcionamiento\n",
    "print(\"Resumen de los trabajadores activos:\")\n",
    "print(client)  # Resumen detallado del cliente\n",
    "\n",
    "print(\"\\nURL del panel de control de Dask (Dashboard):\")\n",
    "print(client.dashboard_link)  # Enlace al dashboard de Dask\n",
    "\n",
    "# Información adicional sobre los trabajadores\n",
    "workers_info = client.scheduler_info()['workers']\n",
    "print(\"\\nInformación detallada de los trabajadores:\")\n",
    "for worker, info in workers_info.items():\n",
    "    print(f\"Trabajador: {worker}\")\n",
    "    print(f\"  Estado: {info.get('status', 'N/A')}\")\n",
    "    print(f\"  Hilos: {info.get('nthreads', 'N/A')}\")\n",
    "    print(f\"  Memoria usada (aproximada): {info.get('metrics', {}).get('memory', 'No disponible')} bytes\")\n",
    "    print(f\"  Memoria límite: {info.get('memory_limit', 'N/A')} bytes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Sección 1 Delayed\n",
    "Ejercicio 1: Procesamiento de datos \n",
    "\n",
    "1. Genera datos simulados (por ejemplo, ventas diarias) para 10 sucursales durante 365 días.\n",
    "    - Cada sucursal debe tener datos generados aleatoriamente para \"Ingresos\" y \"Costos\".\n",
    "    - Utiliza una función para generar los datos simulados.\n",
    "2. Usa Dask Delayed para calcular:\n",
    "    - Las ganancias diarias por sucursal.\n",
    "    - La sucursal con mayor ganancia promedio.\n",
    "3. Genera un grafo de tareas que visualice estas operaciones y explica por qué elegiste paralelizar de esa forma, genera una visualización del grafo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucursal con mayor ganancia promedio: 2 con ganancia promedio de 1379.75\n"
     ]
    }
   ],
   "source": [
    "# Tu código va aquí\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask import delayed\n",
    "import dask.array as da\n",
    "\n",
    "# Paso 1: Generar datos simulados\n",
    "def generate_branch_data(branch_id, days=365):\n",
    "    np.random.seed(branch_id)  # Semilla para reproducibilidad\n",
    "    ingresos = np.random.randint(1000, 5000, days)  # Ingresos diarios\n",
    "    costos = np.random.randint(500, 3000, days)  # Costos diarios\n",
    "    return ingresos, costos\n",
    "\n",
    "# Paso 2: Función para calcular ganancias diarias\n",
    "@delayed\n",
    "def calculate_daily_gains(ingresos, costos):\n",
    "    return ingresos - costos\n",
    "\n",
    "# Paso 3: Función para calcular ganancia promedio de una sucursal\n",
    "@delayed\n",
    "def average_gain(daily_gains):\n",
    "    return daily_gains.mean()\n",
    "\n",
    "# Crear datos para 10 sucursales\n",
    "branches = []\n",
    "for branch_id in range(10):\n",
    "    ingresos, costos = generate_branch_data(branch_id)\n",
    "    daily_gains = calculate_daily_gains(ingresos, costos)\n",
    "    branches.append((branch_id, daily_gains))\n",
    "\n",
    "# Paso 4: Calcular las ganancias promedio por sucursal\n",
    "average_gains = [(branch_id, average_gain(daily_gains)) for branch_id, daily_gains in branches]\n",
    "\n",
    "# Paso 5: Determinar la sucursal con la mayor ganancia promedio\n",
    "@delayed\n",
    "def find_best_branch(avg_gains):\n",
    "    return max(avg_gains, key=lambda x: x[1])  # Seleccionar la sucursal con mayor ganancia promedio\n",
    "\n",
    "best_branch = find_best_branch(average_gains)\n",
    "\n",
    "# Visualizar el grafo de tareas\n",
    "dask.visualize(best_branch, filename='task_graph', format='png')\n",
    "\n",
    "# Computar resultados finales\n",
    "result = best_branch.compute()\n",
    "print(f\"Sucursal con mayor ganancia promedio: {result[0]} con ganancia promedio de {result[1]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 2 Dask Dataframes\n",
    "Ejercicio 2: Limpieza y análisis de datos reales\n",
    "\n",
    "1. Descarga un conjunto de datos masivo (puedes usar la colección de *nycflights* que se encuentra en `data/nycflights/`).\n",
    "2. Carga los datos en un Dask DataFrame. \n",
    "    - Elige adecuadamente el número de particiones (que quepan en memoria de los `workers`)\n",
    "3. Realiza las siguientes tareas:\n",
    "    - Limpia los valores faltantes en las columnas `ArrDelay` y `DepDelay`, rellenándolos con la mediana de cada columna.\n",
    "    - Calcula el retraso promedio (`DepDelay`) por mes y aerolínea.\n",
    "    - Encuentra el aeropuerto de origen con más vuelos retrasados.\n",
    "\n",
    "*Nota*: **Evita** convertir el DataFrame a pandas e **intenta** realizar `.compute()` solo cuando sea necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del DataFrame:\n",
      "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
      "0  1990      1           1          1   1621.0        1540   1747.0   \n",
      "1  1990      1           2          2   1547.0        1540   1700.0   \n",
      "2  1990      1           3          3   1546.0        1540   1710.0   \n",
      "3  1990      1           4          4   1542.0        1540   1710.0   \n",
      "4  1990      1           5          5   1549.0        1540   1706.0   \n",
      "\n",
      "   CRSArrTime UniqueCarrier  FlightNum  ... AirTime  ArrDelay  DepDelay  \\\n",
      "0        1701            US         33  ...     NaN      46.0      41.0   \n",
      "1        1701            US         33  ...     NaN      -1.0       7.0   \n",
      "2        1701            US         33  ...     NaN       9.0       6.0   \n",
      "3        1701            US         33  ...     NaN       9.0       2.0   \n",
      "4        1701            US         33  ...     NaN       5.0       9.0   \n",
      "\n",
      "   Origin  Dest  Distance TaxiIn TaxiOut  Cancelled  Diverted  \n",
      "0     EWR   PIT     319.0    NaN     NaN          0         0  \n",
      "1     EWR   PIT     319.0    NaN     NaN          0         0  \n",
      "2     EWR   PIT     319.0    NaN     NaN          0         0  \n",
      "3     EWR   PIT     319.0    NaN     NaN          0         0  \n",
      "4     EWR   PIT     319.0    NaN     NaN          0         0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Número de particiones ajustado: 4\n",
      "\n",
      "Valores nulos después de la limpieza:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 00:28:04,961 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 763.04 MiB -- Worker memory limit: 0.93 GiB\n",
      "2024-11-21 00:28:04,997 - distributed.worker.memory - WARNING - Worker is at 50% memory usage. Resuming worker. Process memory: 482.25 MiB -- Worker memory limit: 0.93 GiB\n",
      "2024-11-21 00:28:05,250 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 772.85 MiB -- Worker memory limit: 0.93 GiB\n",
      "2024-11-21 00:28:05,298 - distributed.worker.memory - WARNING - Worker is at 57% memory usage. Resuming worker. Process memory: 552.14 MiB -- Worker memory limit: 0.93 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year                       0\n",
      "Month                      0\n",
      "DayofMonth                 0\n",
      "DayOfWeek                  0\n",
      "DepTime                70931\n",
      "CRSDepTime                 0\n",
      "ArrTime                78210\n",
      "CRSArrTime                 0\n",
      "UniqueCarrier              0\n",
      "FlightNum                  0\n",
      "TailNum              1313006\n",
      "ActualElapsedTime      78210\n",
      "CRSElapsedTime          1965\n",
      "AirTime              1356823\n",
      "ArrDelay                   0\n",
      "DepDelay                   0\n",
      "Origin                     0\n",
      "Dest                       0\n",
      "Distance                1495\n",
      "TaxiIn               1313006\n",
      "TaxiOut              1313006\n",
      "Cancelled                  0\n",
      "Diverted                   0\n",
      "dtype: int64\n",
      "\n",
      "Retraso promedio por mes y aerolínea:\n",
      "Month  UniqueCarrier\n",
      "1      AA                8.963504\n",
      "       CO               11.693346\n",
      "       DL                9.249338\n",
      "       EA               24.258191\n",
      "       HP               18.697144\n",
      "                          ...    \n",
      "12     NW                9.705670\n",
      "       PA (1)            8.154915\n",
      "       TW               15.067855\n",
      "       UA                8.790345\n",
      "       US               11.683855\n",
      "Name: DepDelay, Length: 130, dtype: float64\n",
      "\n",
      "Aeropuerto de origen con más vuelos retrasados: EWR\n"
     ]
    }
   ],
   "source": [
    "# Tu código va aquí\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "file_path = 'data/nycflights/*.csv'\n",
    "\n",
    "# Especificar tipos de datos manualmente\n",
    "dtypes = {\n",
    "    'CRSElapsedTime': 'float64',  # Se asegura que sea float64\n",
    "    'TailNum': 'object',         # Se asegura que sea tipo texto\n",
    "    # Agregar más columnas si es necesario con sus tipos correctos\n",
    "}\n",
    "\n",
    "# Cargar los archivos CSV con los tipos de datos especificados\n",
    "df = dd.read_csv(file_path, dtype=dtypes)\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "print(\"Estructura del DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Ajustar particiones para que cada una sea manejable por los workers\n",
    "df = df.repartition(npartitions=4)  # Ajusta el número según tu memoria disponible\n",
    "print(f\"Número de particiones ajustado: {df.npartitions}\")\n",
    "\n",
    "# Paso 2: Limpieza de valores faltantes\n",
    "# Calcular la mediana aproximada de ArrDelay y DepDelay\n",
    "arr_delay_median = df['ArrDelay'].quantile(0.5).compute()\n",
    "dep_delay_median = df['DepDelay'].quantile(0.5).compute()\n",
    "\n",
    "# Rellenar valores faltantes con la mediana aproximada\n",
    "df['ArrDelay'] = df['ArrDelay'].fillna(arr_delay_median)\n",
    "df['DepDelay'] = df['DepDelay'].fillna(dep_delay_median)\n",
    "\n",
    "# Verificar que no haya valores nulos restantes\n",
    "print(\"\\nValores nulos después de la limpieza:\")\n",
    "print(df.isnull().sum().compute())\n",
    "\n",
    "# Paso 3: Análisis\n",
    "# Retraso promedio (DepDelay) por mes y aerolínea\n",
    "monthly_delay = df.groupby(['Month', 'UniqueCarrier'])['DepDelay'].mean().compute()\n",
    "print(\"\\nRetraso promedio por mes y aerolínea:\")\n",
    "print(monthly_delay)\n",
    "\n",
    "# Aeropuerto de origen con más vuelos retrasados\n",
    "# Definir vuelo retrasado como DepDelay > 15 minutos\n",
    "delayed_flights = df[df['DepDelay'] > 15]\n",
    "most_delayed_origin = delayed_flights.groupby('Origin')['DepDelay'].count().idxmax().compute()\n",
    "print(f\"\\nAeropuerto de origen con más vuelos retrasados: {most_delayed_origin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 3 Dask Arrays\n",
    "\n",
    "Ejercicio 3: Procesamiento numérico avanzado\n",
    "\n",
    "1. Crea un arreglo de 10,000 x 10,000 con valores aleatorios usando Dask Array, utiliza un tamaño de chunks adecuado, ¿es mejor que sean cuadrados?.\n",
    "2. Realiza las siguientes operaciones:\n",
    "    - Calcula la suma de cada fila.\n",
    "    - Encuentra la fila con el valor máximo promedio.\n",
    "    - Multiplica todo el arreglo por un factor escalar (por ejemplo, 2.5).\n",
    "3. Divide el arreglo nuevamente en 100 bloques y compara la rapidez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arreglo creado:\n",
      "dask.array<random_sample, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>\n",
      "\n",
      "Suma de cada fila (primeros 10 valores):\n",
      "[4971.69528931 4986.50806879 4980.62337409 5002.45047915 5059.80285318\n",
      " 4991.81024332 4923.59371675 5011.87357736 5014.01459189 4989.76899092]\n",
      "\n",
      "Índice de la fila con el valor máximo promedio: 5788\n",
      "\n",
      "Reconfiguración de chunks completada en 0.00 segundos.\n",
      "\n",
      "Arreglo reconfigurado:\n",
      "dask.array<mul, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "# Tu código va aquí\n",
    "import dask.array as da\n",
    "import time\n",
    "\n",
    "# Paso 1: Crear un arreglo 10,000 x 10,000 con valores aleatorios\n",
    "shape = (10000, 10000)\n",
    "#escogi chunks de 1000, porque dijero que lo optimo era entre 10. \n",
    "#es una elección adecuada ya que balancea la carga de trabajo y se ajusta bien a la memoria disponible\n",
    "#Si conviene usar chunks cuadrados, \n",
    "#Chunks cuadrados distribuyen las operaciones de manera uniforme entre los trabajadores.\n",
    "#Esto ayuda a evitar que algunos chunks sean más pesados de procesar que otros.\n",
    "#Para operaciones como multiplicaciones, sumas o transformaciones de matrices, \n",
    "#los chunks cuadrados son ideales porque minimizan las dependencias entre bloques.\n",
    "chunk_size = (1000, 1000)  # Chunks cuadrados de 1000 x 1000\n",
    "array = da.random.random(shape, chunks=chunk_size)\n",
    "\n",
    "print(\"Arreglo creado:\")\n",
    "print(array)\n",
    "\n",
    "# Paso 2: Calcular la suma de cada fila\n",
    "row_sums = array.sum(axis=1)\n",
    "\n",
    "# Computar la suma de las filas y mostrar la forma del resultado\n",
    "row_sums_computed = row_sums.compute()\n",
    "print(\"\\nSuma de cada fila (primeros 10 valores):\")\n",
    "print(row_sums_computed[:10])\n",
    "\n",
    "# Paso 3: Encontrar la fila con el valor máximo promedio\n",
    "row_means = array.mean(axis=1)\n",
    "max_row_index = row_means.argmax().compute()\n",
    "print(f\"\\nÍndice de la fila con el valor máximo promedio: {max_row_index}\")\n",
    "\n",
    "# Paso 4: Multiplicar todo el arreglo por un factor escalar\n",
    "scalar = 2.5\n",
    "scaled_array = array * scalar\n",
    "\n",
    "# Paso 5: Dividir el arreglo en 100 bloques y comparar rapidez\n",
    "# Reconfigurar chunks en 100 bloques\n",
    "new_chunk_size = (array.shape[0] // 10, array.shape[1] // 10)  # Divide en bloques de 100 x 100\n",
    "start_time = time.time()\n",
    "rechunked_array = scaled_array.rechunk(new_chunk_size)\n",
    "print(f\"\\nReconfiguración de chunks completada en {time.time() - start_time:.2f} segundos.\")\n",
    "\n",
    "# Mostrar el arreglo reconfigurado\n",
    "print(\"\\nArreglo reconfigurado:\")\n",
    "print(rechunked_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección 4 Futures\n",
    "Ejercicio 4: Distribución de tareas dinámicas\n",
    "\n",
    "1. Implementa una función que calcule la raíz cuadrada de una lista de 100,000 números enteros generados aleatoriamente.\n",
    "2. Divide la lista en 10 partes iguales y usa Dask Futures para calcular la raíz cuadrada de cada parte en paralelo.\n",
    "3. Recolecta los resultados y calcula:\n",
    "    - El promedio de todos los números procesados.\n",
    "    - El tiempo total de ejecución (incluyendo envío y recolección de tareas).\n",
    "4. Observa como se distribuye la carga en el cliente.\n",
    "\n",
    "*Nota*: en los ejercicios ya vimos como determinar si ya se cumplío una tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33367 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de los números procesados: 21.09\n",
      "Tiempo total de ejecución: 0.05 segundos\n",
      "Carga distribuida en el cliente: Consulta el dashboard en el URL proporcionado.\n"
     ]
    }
   ],
   "source": [
    "# Tu código va aquí\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n",
    "# Paso 1: Configurar el cliente de Dask\n",
    "client = Client()\n",
    "\n",
    "# Paso 2: Generar una lista de 100,000 números enteros aleatorios\n",
    "n_numbers = 100_000\n",
    "numbers = np.random.randint(1, 1000, size=n_numbers)\n",
    "\n",
    "# Paso 3: Dividir la lista en 10 partes iguales\n",
    "n_chunks = 10\n",
    "chunk_size = n_numbers // n_chunks\n",
    "chunks = [numbers[i * chunk_size:(i + 1) * chunk_size] for i in range(n_chunks)]\n",
    "\n",
    "# Paso 4: Definir la función para calcular la raíz cuadrada\n",
    "def compute_sqrt(data):\n",
    "    return np.sqrt(data)\n",
    "\n",
    "# Paso 5: Enviar tareas a Dask con Futures\n",
    "start_time = time.time()\n",
    "futures = [client.submit(compute_sqrt, chunk) for chunk in chunks]\n",
    "\n",
    "# Paso 6: Recolectar resultados\n",
    "results = client.gather(futures)\n",
    "\n",
    "# Calcular el tiempo total de ejecución\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Paso 7: Calcular el promedio de todos los números procesados\n",
    "all_sqrts = np.concatenate(results)\n",
    "mean_result = all_sqrts.mean()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Promedio de los números procesados: {mean_result:.2f}\")\n",
    "print(f\"Tiempo total de ejecución: {total_time:.2f} segundos\")\n",
    "print(\"Carga distribuida en el cliente: Consulta el dashboard en el URL proporcionado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
